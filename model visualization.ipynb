{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73794c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ../envsim/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f451b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: transformers in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/users/ocagatan19/envsim/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08392f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (1.13.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp38-cp38-linux_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 18 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: requests in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (49.2.1)\n",
      "Requirement already satisfied: wheel in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "torchvision 0.14.1+cu116 requires torch==1.13.1, but you'll have torch 1.13.0 which is incompatible.\n",
      "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you'll have torch 1.13.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/users/ocagatan19/envsim/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31609642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: tensorboard in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (49.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (1.51.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from tensorboard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (5.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /scratch/users/ocagatan19/envsim/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/scratch/users/ocagatan19/envsim/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d357995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoConfig,TrainingArguments\n",
    "from simcse.model.bert_roberta_models import BertForBarlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a1bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce447cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confing = AutoConfig.from_pretrained(\"experiments/unsup-barlow/unsup-barlow-bert-base-uncased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0e898ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd82ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daaf7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from transformers.file_utils import (\n",
    "    cached_property,\n",
    "    torch_required,\n",
    "    is_torch_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cdd40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BarlowTrainingArguments(TrainingArguments):\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate\n",
    "    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n",
    "    ## both STS and transfer tasks (dev) during training.\n",
    "    eval_transfer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"},\n",
    "    )\n",
    "\n",
    "    augmentation_method: str = field(\n",
    "        default=\"dropout\", metadata={\"help\": \"Augmentation method to create positive pairs for unsupervised training.\"}\n",
    "    )\n",
    "\n",
    "    lambd: float = field(\n",
    "        default= 0.01, metadata={\"help\": \"Weight for covariance loss\"},\n",
    "    )\n",
    "\n",
    "\n",
    "    do_proj: bool = field(\n",
    "        default=False, metadata={\"help\": \"Boolean value to add MLP\"}\n",
    "    )\n",
    "    batch_size:int = field(\n",
    "        default=512, metadata = {\"help\": \"Integer value to the \"}\n",
    "    )\n",
    "\n",
    "    freeze_bert: bool = field(\n",
    "        default=False, metadata={\"help\": \"Boolean value to freeze BERT\"}\n",
    "    )\n",
    "\n",
    "    proj_output_dim: str = field(\n",
    "        default=\"4096-4096-768\", metadata={\"help\": \"Projection output dimension\"}\n",
    "    )\n",
    "\n",
    "\n",
    "    @cached_property\n",
    "    @torch_required\n",
    "    def _setup_devices(self) -> \"torch.device\":\n",
    "        logger.info(\"PyTorch: setting up devices\")\n",
    "        if self.no_cuda:\n",
    "            device = torch.device(\"cpu\")\n",
    "            self._n_gpu = 0\n",
    "        elif is_torch_tpu_available():\n",
    "            device = xm.xla_device()\n",
    "            self._n_gpu = 0\n",
    "        elif self.local_rank == -1:\n",
    "            # if n_gpu is > 1 we'll use nn.DataParallel.\n",
    "            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n",
    "            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n",
    "            # trigger an error that a device index is missing. Index 0 takes into account the\n",
    "            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n",
    "            # will use the first GPU in that env, i.e. GPU#1\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n",
    "            # the default value.\n",
    "            self._n_gpu = torch.cuda.device_count()\n",
    "        else:\n",
    "            # Here, we'll use torch.distributed.\n",
    "            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n",
    "            #\n",
    "            # deepspeed performs its own DDP internally, and requires the program to be started with:\n",
    "            # deepspeed  ./program.py\n",
    "            # rather than:\n",
    "            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n",
    "            if self.deepspeed:\n",
    "                from .integrations import is_deepspeed_available\n",
    "\n",
    "                if not is_deepspeed_available():\n",
    "                    raise ImportError(\n",
    "                        \"--deepspeed requires deepspeed: `pip install deepspeed`.\"\n",
    "                    )\n",
    "                import deepspeed\n",
    "\n",
    "                deepspeed.init_distributed()\n",
    "            else:\n",
    "                torch.distributed.init_process_group(backend=\"nccl\")\n",
    "            device = torch.device(\"cuda\", self.local_rank)\n",
    "            self._n_gpu = 1\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.set_device(device)\n",
    "\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "904acb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MODEL_FOR_MASKED_LM_MAPPING\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5868db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Huggingface's original arguments\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    hidden_dropout_prob: Optional[float] = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\":\"Dropout probability for generating positive pairs.\"}\n",
    "    )\n",
    "\n",
    "    loss_elements: Optional[str] = field(\n",
    "        default=\"cov-inv\",\n",
    "        metadata={\"help\":\"Sub-loss calculations to form total loss.\"}\n",
    "    )\n",
    "\n",
    "    tensorboard_path: Optional[str] = field(\n",
    "        default=\"tensorboard\",\n",
    "        metadata={\"help\":\"Path to save tansorboard trackings.\"}\n",
    "    )\n",
    "\n",
    "    tensorboard_save_frequency: Optional[int] = field(\n",
    "        default=100,\n",
    "        metadata={\"help\":\"Frequency to save tensorboard info.\"}\n",
    "    )\n",
    "\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"If training from scratch, pass a model type from the list: \"\n",
    "            + \", \".join(MODEL_TYPES)\n",
    "        },\n",
    "    )\n",
    "    ssl_type: Optional[str] = field(default=\"simcse\",)\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"\n",
    "        },\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # SimCSE's arguments\n",
    "    temp: float = field(default=0.05, metadata={\"help\": \"Temperature for softmax.\"})\n",
    "    pooler_type: str = field(\n",
    "        default=\"cls\",\n",
    "        metadata={\n",
    "            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n",
    "        },\n",
    "    )\n",
    "    hard_negative_weight: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n",
    "        },\n",
    "    )\n",
    "    do_mlm: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use MLM auxiliary objective.\"}\n",
    "    )\n",
    "    mlm_weight: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\n",
    "            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n",
    "        },\n",
    "    )\n",
    "    mlp_only_train: bool = field(\n",
    "        default=False, metadata={\"help\": \"Use MLP only during training\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    # Huggingface's original arguments.\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The configuration name of the dataset to use (via the datasets library).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "    # SimCSE's arguments\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The training data file (.txt or .csv).\"}\n",
    "    )\n",
    "\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=32,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15,\n",
    "        metadata={\n",
    "            \"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if (\n",
    "            self.dataset_name is None\n",
    "            and self.train_file is None\n",
    "            and self.validation_file is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Need either a dataset name or a training/validation file.\"\n",
    "            )\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\n",
    "                    \"csv\",\n",
    "                    \"json\",\n",
    "                    \"txt\",\n",
    "                ], \"`train_file` should be a csv, a json or a txt file.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d6ce559",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = torch.load(\"experiments/unsup-barlow/unsup-barlow-bert-base-uncased/training_args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f557d64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BarlowTrainingArguments(output_dir='experiments/unsup-barlow-bert-base-uncased', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=256, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=1e-06, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=1000, log_level='passive', log_level_replica='passive', log_on_each_node=True, logging_dir='experiments/unsup-barlow-bert-base-uncased/runs/Dec23_00-47-23_rk02.kuacc.ku.edu.tr', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=None, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=150, dataloader_num_workers=0, past_index=-1, run_name='experiments/unsup-barlow-bert-base-uncased', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, eval_transfer=False, augmentation_method='dropout', lambd=0.0051, do_proj=True, batch_size=1024, freeze_bert=False, proj_output_dim='8192-8192-8192')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43ef889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59e5ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments,BarlowTrainingArguments)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01edee1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-f', '--fff'], dest='fff', nargs=None, const=None, default='1', type=None, choices=None, help='a dummy argument to fool ipython', metavar=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3474df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH] [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                             [--loss_elements LOSS_ELEMENTS] [--tensorboard_path TENSORBOARD_PATH]\n",
      "                             [--tensorboard_save_frequency TENSORBOARD_SAVE_FREQUENCY] [--model_type MODEL_TYPE] [--ssl_type SSL_TYPE]\n",
      "                             [--config_name CONFIG_NAME] [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                             [--use_fast_tokenizer [USE_FAST_TOKENIZER]] [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]\n",
      "                             [--use_auth_token [USE_AUTH_TOKEN]] [--temp TEMP] [--pooler_type POOLER_TYPE]\n",
      "                             [--hard_negative_weight HARD_NEGATIVE_WEIGHT] [--do_mlm [DO_MLM]] [--mlm_weight MLM_WEIGHT]\n",
      "                             [--mlp_only_train [MLP_ONLY_TRAIN]] [--dataset_name DATASET_NAME] [--dataset_config_name DATASET_CONFIG_NAME]\n",
      "                             [--overwrite_cache [OVERWRITE_CACHE]] [--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]\n",
      "                             [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS] [--train_file TRAIN_FILE] [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                             [--pad_to_max_length [PAD_TO_MAX_LENGTH]] [--mlm_probability MLM_PROBABILITY] --output_dir OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]] [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]] [--evaluation_strategy {no,steps,epoch}] [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE] [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE] [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE] [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON] [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS] [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                             [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS] [--log_level {debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {debug,info,warning,error,critical,passive}] [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node] [--logging_dir LOGGING_DIR] [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]] [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]] [--no_logging_nan_inf_filter] [--save_strategy {no,steps,epoch}]\n",
      "                             [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT] [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED] [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]] [--bf16 [BF16]] [--fp16 [FP16]]\n",
      "                             [--fp16_opt_level FP16_OPT_LEVEL] [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]] [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32] [--local_rank LOCAL_RANK]\n",
      "                             [--xpu_backend {mpi,ccl,gloo}] [--tpu_num_cores TPU_NUM_CORES] [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]] [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS] [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM] [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]] [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]] [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL] [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]] [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS] [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--deepspeed DEEPSPEED] [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,adamw_anyprecision,sgd,adagrad}]\n",
      "                             [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]] [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME] [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS] [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]] [--no_dataloader_pin_memory]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]] [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]] [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT] [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}] [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo [HUB_PRIVATE_REPO]] [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]] [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID] [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN] [--mp_parameters MP_PARAMETERS] [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n",
      "                             [--ray_scope RAY_SCOPE] [--ddp_timeout DDP_TIMEOUT] [--eval_transfer [EVAL_TRANSFER]]\n",
      "                             [--augmentation_method AUGMENTATION_METHOD] [--lambd LAMBD] [--do_proj [DO_PROJ]] [--batch_size BATCH_SIZE]\n",
      "                             [--freeze_bert [FREEZE_BERT]] [--proj_output_dim PROJ_OUTPUT_DIM] [-f FFF]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    " model_args, data_args, training_args = parser.parse_args_into_dataclasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f84cc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForBarlow(config=confing,training_args=training_args,model_args = ModelArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a6ecfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"experiments/unsup-barlow/unsup-barlow-bert-base-uncased/pytorch_model.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57bf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
